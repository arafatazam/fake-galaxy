{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQrDDC-QoAX6"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "GpNJxY1eoJP3",
        "outputId": "4f0e4b02-c45c-4f70-95b3-c0595cb55b83"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-60e126a2aecc>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_real_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_real_images.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain_fake_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_fake_images.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_real_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_real_images.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_fake_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_fake_images.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_real_images.npy'"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define the data generators for training and testing data\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,  # normalize pixel values to [0,1]\n",
        "    rotation_range=20,  # randomly rotate images up to 20 degrees\n",
        "    width_shift_range=0.2,  # randomly shift images horizontally by up to 20% of the image width\n",
        "    height_shift_range=0.2,  # randomly shift images vertically by up to 20% of the image height\n",
        "    shear_range=0.2,  # randomly apply shearing transformations\n",
        "    zoom_range=0.2,  # randomly zoom into images by up to 20%\n",
        "    horizontal_flip=True,  # randomly flip images horizontally\n",
        "    vertical_flip=True  # randomly flip images vertically\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Define the directories for the training and testing data\n",
        "train_dir = 'path/to/training/directory'\n",
        "test_dir = 'path/to/testing/directory'\n",
        "\n",
        "# Generate batches of image data from the directories using the data generators\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,  # path to the training directory\n",
        "    target_size=(256, 256),  # resize images to 256x256 pixels\n",
        "    batch_size=32,\n",
        "    class_mode='binary'  # set to 'binary' for binary classification\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,  # path to the testing directory\n",
        "    target_size=(256, 256),  # resize images to 256x256 pixels\n",
        "    batch_size=32,\n",
        "    class_mode='binary'  # set to 'binary' for binary classification\n",
        ")\n",
        "\n",
        "\n",
        "# Load the data\n",
        "train_real_images = np.load('train_real_images.npy')\n",
        "train_fake_images = np.load('train_fake_images.npy')\n",
        "test_real_images = np.load('test_real_images.npy')\n",
        "test_fake_images = np.load('test_fake_images.npy')\n",
        "\n",
        "# Rescale the pixel values to be between 0 and 1\n",
        "train_real_images = train_real_images / 255.0\n",
        "train_fake_images = train_fake_images / 255.0\n",
        "test_real_images = test_real_images / 255.0\n",
        "test_fake_images = test_fake_images / 255.0\n",
        "\n",
        "# Resize the images to a consistent size\n",
        "IMG_SIZE = 128\n",
        "train_real_images = tf.image.resize(train_real_images, (IMG_SIZE, IMG_SIZE))\n",
        "train_fake_images = tf.image.resize(train_fake_images, (IMG_SIZE, IMG_SIZE))\n",
        "test_real_images = tf.image.resize(test_real_images, (IMG_SIZE, IMG_SIZE))\n",
        "test_fake_images = tf.image.resize(test_fake_images, (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "# Combine the real and fake images for training and testing\n",
        "train_images = tf.concat([train_real_images, train_fake_images], axis=0)\n",
        "test_images = tf.concat([test_real_images, test_fake_images], axis=0)\n",
        "train_labels = np.concatenate([np.ones(len(train_real_images)), np.zeros(len(train_fake_images))])\n",
        "test_labels = np.concatenate([np.ones(len(test_real_images)), np.zeros(len(test_fake_images))])\n"
      ],
      "metadata": {
        "id": "GpNJxY1eoJP3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "ck5WqmgOo9T2",
        "outputId": "11d8c371-ac4f-41bf-ab8d-76879ebb7133"
      },
      "execution_count": null,
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b64a97fb11bf>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-b64a97fb11bf>\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   model = models.Sequential([\n\u001b[0;32m----> 3\u001b[0;31m       \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m       \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'IMG_SIZE' is not defined"
          ]
        }
      ],
      "source": [
        "def create_model():\n",
        "  model = models.Sequential([\n",
        "      layers.Conv2D(32, (3,3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
        "      layers.MaxPooling2D((2,2)),\n",
        "      layers.Conv2D(64, (3,3), activation='relu'),\n",
        "      layers.MaxPooling2D((2,2)),\n",
        "      layers.Conv2D(128, (3,3), activation='relu'),\n",
        "      layers.MaxPooling2D((2,2)),\n",
        "      layers.Flatten(),\n",
        "      layers.Dense(64, activation='relu'),\n",
        "      layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "model = create_model()\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "  model = models.Sequential([\n",
        "      layers.Conv2D(32, (3,3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
        "      layers.MaxPooling2D((2,2)),\n",
        "      layers.Conv2D(64, (3,3), activation='relu'),\n",
        "      layers.MaxPooling2D((2,2)),\n",
        "      layers.Conv2D(128, (3,3), activation='relu'),\n",
        "      layers.MaxPooling2D((2,2)),\n",
        "      layers.Flatten(),\n",
        "      layers.Dense(64, activation='relu'),\n",
        "      layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "model = create_model()\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "ck5WqmgOo9T2",
        "outputId": "11d8c371-ac4f-41bf-ab8d-76879ebb7133"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b64a97fb11bf>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-b64a97fb11bf>\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   model = models.Sequential([\n\u001b[0;32m----> 3\u001b[0;31m       \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m       \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'IMG_SIZE' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))\n"
      ],
      "metadata": {
        "id": "UICItD_dobFp"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test saved model\n",
        "\n",
        "loaded_model = tf.keras.models.load_model('/content/Group_X_Model_Trained')\n",
        "loaded_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCEZ79GPotZ0",
        "outputId": "b7cc8c06-70ff-4b00-ec54-5e3aeed81241"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 65\n",
            "Trainable params: 65\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
        "print('Test accuracy:', test_acc)\n",
        "\n",
        "predictions = model.predict(test_images)\n",
        "predictions = np.round(predictions)\n",
        "confusion_matrix = tf.math.confusion_matrix(test_labels, predictions)\n",
        "print('Confusion matrix:')\n",
        "print(confusion_matrix.numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygEz3IZ1qqS5",
        "outputId": "6743d822-a5e5-44f9-865f-fc46f29a428f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/Group_X_Model_Trained/ (stored 0%)\n",
            "  adding: content/Group_X_Model_Trained/variables/ (stored 0%)\n",
            "  adding: content/Group_X_Model_Trained/variables/variables.index (deflated 51%)\n",
            "  adding: content/Group_X_Model_Trained/variables/variables.data-00000-of-00001 (deflated 54%)\n",
            "  adding: content/Group_X_Model_Trained/saved_model.pb (deflated 85%)\n",
            "  adding: content/Group_X_Model_Trained/assets/ (stored 0%)\n",
            "  adding: content/Group_X_Model_Trained/keras_metadata.pb (deflated 77%)\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
        "print('Test accuracy:', test_acc)\n",
        "\n",
        "predictions = model.predict(test_images)\n",
        "predictions = np.round(predictions)\n",
        "confusion_matrix = tf.math.confusion_matrix(test_labels, predictions)\n",
        "print('Confusion matrix:')\n",
        "print(confusion_matrix.numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IXV8ZfQ3qcR"
      },
      "outputs": [],
      "source": [
        "model.save('my_model')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('my_model')\n"
      ],
      "metadata": {
        "id": "3IXV8ZfQ3qcR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}